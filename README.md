# âš¡ Zentro â€“ Intelligent Content Management v1.0

![Zentro Banner](https://img.shields.io/badge/Zentro-v1.0-magenta?style=flat-square)
![Engine](https://img.shields.io/badge/Engine-Cyclops--VL%202.0%20%2B%20CUDA-blue?style=flat-square)
![Creator](https://img.shields.io/badge/Creator-ALAN%20CYRIL%20SUNNY-green?style=flat-square)
![Python](https://img.shields.io/badge/Language-Python%203.10%2B-blue)
![GPU](https://img.shields.io/badge/Optimized-RTX%202050--4090-orange?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-blue?style=flat-square)

> **Open-source | Offline-first | RTX-optimized**  
> If this helps, please â­ [star the repository](https://github.com/dragonpilee/zentro)!

---

## ğŸ§  Zentro â€” Overview

Zentro is an offline, GPU-accelerated platform for image & document intelligence and retrieval-augmented conversational AI. Built for local RTX GPUs and powered by Cyclops-VL 2.0 â€” an in-house vision-language model â€” it provides fast on-device reasoning, document understanding, and RAG workflows.

Key pillars:
- Local-first, privacy-preserving
- CUDA & mixed-precision inference
- Optimized for NVIDIA RTX GPUs

---

## âœ¨ Core Features

- Zentro Vision â€” object, text, layout detection, diagram analysis, image summarization  
- Zentro Docs â€” PDF/TXT parsing, metadata extraction, structured summaries  
- Zentro Chat â€” offline RAG with GPU embeddings, cosine retrieval, Cyclops-VL 2.0 grounding  
- Fast GPU embeddings, TensorRT-friendly paths, AMP mixed precision

---

## âš¡ GPU Optimization (Open-Source)

- CUDA-accelerated inference and embeddings  
- Mixed Precision (AMP) support  
- TensorRT-friendly model flow  
- Optimized for RTX 2050 â†’ 4090 & A-series

---

## ğŸ§© Tech Stack

| Component | Technology |
|---|---|
| UI | Streamlit |
| Backend | FastAPI (OpenAI-compatible endpoints) |
| V-L Model | Cyclops-VL 2.0 (in-house) |
| Embeddings | SentenceTransformers (GPU) |
| Retrieval | Cosine similarity (local) |
| Parsing | PyPDF / text utilities |
| Deployment | Conda / uvicorn / Streamlit |

---

## ğŸ’» Requirements

- Python 3.10+  
- NVIDIA GPU with CUDA (tested RTX series)  
- Conda/Mamba recommended  
- Optional: TensorRT for further speedups

---

## ğŸ“¦ Quick Install (recommended)

```powershell
mamba create -n zentro -c conda-forge python=3.10 -y
conda activate zentro
pip install -r requirements.txt
# Optional: install cupy/cuda toolkit per your CUDA version
```

---

## ğŸš€ Run Locally (Offline)

Start backend:
```powershell
uvicorn backend:app --reload --host 127.0.0.1 --port 8000
```

Health:
```
http://127.0.0.1:8000/health
```

Start frontend:
```powershell
streamlit run streamlit_app.py
```

UI:
```
http://localhost:8501
```

Backend URL (default):
```
http://127.0.0.1:8000
```

---

## ğŸ–¼ Screenshots 

![Screenshot 1](1.png)  
![Screenshot 2](2.png)  
![Screenshot 3](3.png)


---

## ğŸ” Privacy & Modes

- Offline open-source version: fully local, no telemetry, no external APIs.  
- Commercial cloud-managed version (optional): managed cloud service with multi-tenant features, orchestration, centralized storage, RBAC and monitoring. The cloud product is separate from this repo.

---

## ğŸ›  Troubleshooting

- Backend not reachable: ensure uvicorn process is running and port 8000 is free.  
- GPU not detected:
```python
import torch
torch.cuda.is_available()
```
- Chat not returning answers: ensure documents are indexed and embeddings built.

---

## ğŸ“ Project Structure

```
zentro/
â”‚â”€â”€ backend.py
â”‚â”€â”€ streamlit_app.py
â”‚â”€â”€ environment.yml
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ 1.png
â”‚â”€â”€ 2.png
â”‚â”€â”€ 3.png
```

---

## ğŸš€ Future Work

- Advanced OCR & handwriting recognition  
- Knowledge graph extraction & analytics  
- TensorRT end-to-end pipelines  
- Multi-user sync (cloud product)

---

## ğŸ¤ Contributing

Contributions welcome. Follow standard PR workflow and keep changes GPU-friendly and offline-first.

---

## ğŸ“œ License

MIT License â€” free to use, modify, and share. Attribution appreciated.

---

ğŸ’š Built for RTX GPUs â€” local-first,
